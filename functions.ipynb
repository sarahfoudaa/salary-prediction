{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1sJDBOpgYbmE_30Bpz6W01TeGM8NS8DOh","authorship_tag":"ABX9TyO1EpytQEtqX9q06VuLmZRc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SY62uGF-jFQF","executionInfo":{"status":"ok","timestamp":1707009403413,"user_tz":-120,"elapsed":12,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}},"outputId":"7d24cb3e-f66f-4a10-8d5b-df50f252f9eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing functions.py\n"]}],"source":["structure: https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510\n","git: https://neptune.ai/blog/google-colab-dealing-with-files"]},{"cell_type":"code","source":[],"metadata":{"id":"PPGXA-O7zlwf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/machine_learning_projects/salary_prediction/salary-prediction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ImI4SQJsyS1","executionInfo":{"status":"ok","timestamp":1707342271862,"user_tz":-120,"elapsed":402,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}},"outputId":"6ba61d10-1eba-4150-a933-c30ae73c26c1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/machine_learning_projects/salary_prediction/salary-prediction\n"]}]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mxfSyM4_tKt2","executionInfo":{"status":"ok","timestamp":1707342271862,"user_tz":-120,"elapsed":6,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}},"outputId":"7c8d92bb-2bcc-4280-ad63-a7466806c7d2"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/machine_learning_projects/salary_prediction/salary-prediction'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hU8qM87D4rVr","executionInfo":{"status":"ok","timestamp":1707342272641,"user_tz":-120,"elapsed":371,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}},"outputId":"a2008811-343e-4ad5-8112-52d0c892d325"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[" config.py        'Feature Engineering.ipynb'   git.ipynb                 Untitled\n"," custom_funcs.py  'Feature selection.ipynb'    'lazy prediction .ipynb'\n"," EDA.ipynb         functions.ipynb              README.md\n"]}]},{"cell_type":"code","source":["%rm config.py"],"metadata":{"id":"UW7cKZ7V2fXm","executionInfo":{"status":"ok","timestamp":1707342354500,"user_tz":-120,"elapsed":340,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["%%writefile config.py\n","#where you originally get the dataset (sklearn datasets, keras datasets, kaggle....)\n","from pathlib import Path\n","\n","data_dir = '/content/drive/MyDrive/machine_learning_projects/salary_prediction/'\n","data_path = data_dir + 'Salary Prediction of Data Professions.csv'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0P6tIRnzrAU","executionInfo":{"status":"ok","timestamp":1707342367240,"user_tz":-120,"elapsed":388,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}},"outputId":"d059e21a-b099-48a1-ba68-516155c2ee88"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing config.py\n"]}]},{"cell_type":"code","source":["%rm custom_funcs.py"],"metadata":{"id":"lUFfaL4d6FmR","executionInfo":{"status":"ok","timestamp":1707345322567,"user_tz":-120,"elapsed":328,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["%%writefile custom_funcs.py\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import scipy.stats as stats\n","import pylab\n","from config import data_path\n","from sklearn import preprocessing\n","\n","#--------------------------EDA functions--------------------------\n","def data_info(df):\n","  '''\n","  Function that prints some important informations about the raw dataset at the beginning of the EDA select_dtypes\n","\n","  parameters:\n","    df(DataFrame):dataframe of the raw dataset\n","  '''\n","  print('sample of the data \\n', df.head(5),'\\n')\n","  print('data types \\n', df.info(),'\\n \\n')\n","  print('data describtion \\n', df.describe().T,'\\n \\n')\n","  print('unique values for each feature \\n', df.nunique(),'\\n \\n')\n","  print('sum of missing values \\n',df.isna().sum(),'\\n \\n')\n","  print('percentage of missing values \\n', df.isna().mean(),'\\n \\n')\n","  print('skewness of numerical data', df.skew())\n","  sns.heatmap(df.isna(), cbar = False, cmap = 'viridis')\n","\n","\n","def vis(df,feature):\n","  '''\n","  Function that visualize the histplot, boxplot, and violinplot for a specific feature in the dataset to give insight on the distribution, the IQR, and if there is any outliers\n","\n","  parameters:\n","    df(DataFrame):dataframe of the dataset\n","    feature(string): a column in the sent received dataset\n","  '''\n","  sns.set()\n","  plt.figure(figsize = (7,5))\n","  fig, axes = plt.subplots(2, 2,figsize = (15,8))\n","  sns.histplot(x = df[feature],ax=axes[0,0],kde = True,stat = 'density')#https://www.geeksforgeeks.org/how-to-make-histograms-with-density-plots-with-seaborn-histplot/\n","  sns.boxplot(data = df, x= feature,ax=axes[0,1]) #https://www.labxchange.org/library/items/lb:LabXchange:46f64d7a:html:1\n","  sns.violinplot(x = df[feature],ax=axes[1,0])\n","\n","\n","def detect_outliers_iqr(df,features):\n","  '''\n","  Function that calculates the lower and upper bounf of a nuerical feature to then get the outliers that are not within these bounds and prints these outliers values\n","\n","  Parameters:\n","    df(DataFrame):dataframe of the dataset\n","    feature(string): a column in the sent received dataset\n","  '''\n","  outliers = []\n","  before_lwr_bound= []\n","  after_upr_bound =[]\n","  for feature in features:\n","    df[feature] = sorted(df[feature])\n","    q1 = np.percentile(df[feature], 25)\n","    q3 = np.percentile(df[feature], 75)\n","    IQR = q3- q1\n","    lwr_bound = q1 - (1.5*IQR)\n","    upr_bound = q3 + (1.5*IQR)\n","\n","    for i in df[feature]:\n","      if (i<lwr_bound):\n","        outliers.append(i)\n","        before_lwr_bound.append(i)\n","      if (i>upr_bound):\n","        outliers.append(i)\n","        after_upr_bound.append(i)\n","    print(\"-------------------\",feature,\"-------------------\")\n","    print(\"Outliers of \",feature ,\" from IQR method: \", outliers)\n","    print('before lower bound of ',feature ,\" : \", before_lwr_bound)\n","    print('after upper bound of ',feature ,\" : \", after_upr_bound)\n","    sns.histplot(x =outliers)\n","    plt.show()\n","\n","\n","def detect_distriution(df):\n","  '''\n","  Function that detect the skewness from the normal distribution of the numerical features in the dataset and divid them into categories (highly_positively_skewed, highly_negatively_skewed, moderately_positively_skewed, moderately_negatively_skewed, and normally_distributed) which each of them have a range of skewness\n","\n","  parameters:\n","    df(DataFrame):dataframe of the dataset\n","\n","  Returns:\n","    non_normal_features(string): a string that contains all non uniform distributed features\n","  '''\n","  print(df.skew())\n","  highly_positively_skewed = [x for x in df.select_dtypes(include = ['int', 'float']).columns if dict(df.skew(numeric_only=True)>1)[x] == True]\n","  highly_negatively_skewed = [x for x in df.select_dtypes(include = ['int', 'float']).columns if dict(df.skew(numeric_only=True)<-1)[x] == True]\n","  moderately_positively_skewed = [x for x in df.select_dtypes(include = ['int', 'float']).columns if (dict(df.skew(numeric_only=True)>0.5 )[x] == True) and (dict(df.skew(numeric_only=True)<1 )[x] == True)]\n","  moderately_negatively_skewed = [x for x in df.select_dtypes(include = ['int', 'float']).columns if (dict(df.skew(numeric_only=True)>-0.5 )[x] == True) and (dict(df.skew(numeric_only=True)<-1 )[x] == True)]\n","  normally_distributed = [x for x in df.select_dtypes(include = ['int', 'float']).columns if (dict(df.skew(numeric_only=True)>-0.5 )[x] == True) and (dict(df.skew(numeric_only=True)<0.5 )[x] == True)]\n","  print()\n","  print('features that are highly positively skewed (skewness >1) are ',highly_positively_skewed)\n","  print('features that are highly negatively skewed (skewness <-1) are ',highly_negatively_skewed)\n","  print('features that are moderately positively skewed (0.5 < skewness < 1 ) are ',moderately_positively_skewed)\n","  print('features that are moderately negatively skewed (-0.5 < skewness < -1 ) are ',moderately_negatively_skewed)\n","  print('features that are normally distributed (-0.5 < skewness < 0.5 ) are',normally_distributed)\n","  print()\n","\n","  for col in df.select_dtypes(include = ['int', 'float']).columns:\n","    plt.subplot(1,2,1)\n","    sns.violinplot(df[col])\n","    plt.subplot(1,2,2)\n","    sns.histplot(df[col],kde = True,stat = 'density')\n","    plt.show()\n","\n","  non_normal_features = highly_positively_skewed + highly_negatively_skewed + moderately_positively_skewed + moderately_negatively_skewed\n","\n","  print('transformation ',non_normal_features,' to change the distribution')\n","  return non_normal_features\n","\n","\n","def plot_kde_probplot(df,feature,title):\n","  '''\n","  Function that plots the kdeplot and subplot to them give us insight of which transformationis better for this feature from the plotting and the calculations of the mean and std\n","\n","  parameters:\n","    df(DataFrame):dataframe of the dataset\n","    feature(string): a column in the sent received dataset\n","    title(string): a string that indicates what transformation are we applying for visualization\n","  '''\n","  plt.figure(figsize = (5,3))\n","  plt.subplot(1,2,1)\n","  sns.kdeplot(df[feature])\n","  plt.subplot(1,2,2)\n","  stats.probplot(df[feature], plot = pylab)\n","  print('mean = ',df[feature].mean(),' std = ',df[feature].std())\n","  plt.title(title)\n","  plt.show()\n","\n","\n","def normality(df, feature):#original, log, reciprocal, square root, and exponential\n","  '''\n","  Function that calls for the plot_kde_probplot function to plot different types of transformations( log, reciprocal, square root, and exponential) for a scpecific feature to give us insight which is the best according to the ploting and calculations displayed\n","\n","  Parameters:\n","    df(DataFrame):dataframe of the dataset\n","    feature(string): a column in the sent received dataset\n","  '''\n","  #original\n","  plot_kde_probplot(df,feature,'original')\n","  print(df[feature].skew(),'\\n')\n","\n","  #log\n","  test = pd.DataFrame(np.log(df[feature].dropna()))\n","  plot_kde_probplot(test, feature,'log')\n","  print(test.skew(),'\\n')\n","\n","  #reciprocal\n","  test = pd.DataFrame(1/df[feature].dropna())\n","  plot_kde_probplot(test, feature,'reciprotical')\n","  print(test.skew(),'\\n')\n","\n","  #square root\n","  test = pd.DataFrame(np.sqrt(df[feature].dropna()))\n","  plot_kde_probplot(test, feature,'square root')\n","  print(test.skew(),'\\n')\n","\n","  #exponential\n","  test = pd.DataFrame(df[feature].dropna()**(1/1.2))\n","  plot_kde_probplot(test,feature,'exponential')\n","  print(test.skew(),'\\n')\n","\n","\n","\n","#--------------------------feature engineering funcitons--------------------------\n","def split(ratio):\n","  '''\n","  Function that splits a dataframe into train dataset and test according to a specific given ratio\n","\n","  Parameters:\n","  ratio(int): the ratio the dataset is going to be splitted into\n","\n","  Returns:\n","   df(DataFrame): a copy of the original dataset\n","   X_train(DataFrame): train dataset that will be feature engineered\n","   X_test(DataFrame): test datset that wont be feature engineered as it will be used in the evaluation step to evaluate the model(S)\n","  '''\n","  df = pd.read_csv(data_path)\n","  X_train, X_test  = train_test_split(df, test_size = ratio, random_state = 41)\n","  X_train.reset_index(inplace = True, drop=True)\n","  X_test.reset_index(inplace = True, drop=True)\n","\n","  return df,X_train, X_test\n","\n","\n","def splitModeling(X_train,y_train, ratio ):\n","  '''\n","  Function that splits the dataset after the feature engineering and feature selection steps into X_train, X_test, y_train, y_tes\n","\n","  Parameters:\n","    X_train(Dataframe): clean train dataset\n","    y_train(Dataframe): dataset's label or target\n","    ratio(int): the ratio the dataset is going to be splitted into\n","\n","  Returns:\n","    X_train(Dataframe): train datset that is ready for modeling\n","    X_test(Dataframe): test datset that is ready for testing\n","    y_train(Dataframe): label of train\n","    y_test(Dataframe): label of test\n","  '''\n","  X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = ratio, random_state = 41)\n","  return X_train, X_test, y_train, y_test\n","\n","\n","def subtract_nan(row):\n","  '''\n","  Funciton that handels missing values in the two features LEAVES USED and LEAVES REMAINING\n","\n","  Parameters:\n","    row(Dataframe)\n","\n","  Return:\n","    row(Dataframe)\n","  '''\n","  if np.isnan(row['LEAVES USED']):\n","    row['LEAVES USED'] = 30 - row['LEAVES REMAINING']\n","  if np.isnan(row['LEAVES REMAINING']):\n","    row['LEAVES REMAINING'] = 30 - row['LEAVES USED']\n","\n","  return row\n","\n","\n","#after handelling all missing values\n","def type_casting(df,data_types_dict):\n","  '''\n","  Funciton that type cast all the features that not in the right datatype\n","\n","  Parameters:\n","    df(dataframe): dataset that the needs type casting\n","    data_types_dict(dictionary): a dictionary that contains datatype as the keys and corresponding to them the features to be casted\n","  '''\n","  if(data_types_dict['int']):\n","    df[data_types_dict['int']] = df[data_types_dict['int']].astype(int)\n","\n","  for feature in df[data_types_dict['datetime']]:\n","    df[feature] = pd.to_datetime(df[feature])\n","\n","\n","def convert_dates_to_duration(df, starts, ends):\n","  '''\n","  Function the calculated the duration in between two given datset\n","\n","  Parameters:\n","    df(Dataframe)\n","    starts(datetime): that start date to the event\n","    ends(datetime): that end date to the event\n","  '''\n","  df['nb_months'] = ((df[ends] - df[starts])/np.timedelta64(1,'M')).astype(int)\n","  df['yearsMonths'] = df['nb_months'].apply(lambda x: int(x/12) + (x%12)/12)\n","  df.drop(columns = ['DOJ', 'CURRENT DATE'],inplace = True)\n","\n","\n","def past_exp_binning(x):\n","  '''\n","  Funciton that segments or bins numerical feature into specific segments and change it into categorical\n","\n","  Parameters:\n","    x(int): the values to be replaced with a bins\n","\n","  Returns:\n","    the bin in will be in\n","  '''\n","  x = int(abs(x))\n","  if (x==0):\n","    return \"0\"\n","  elif (1 <= x <= 3):\n","    return \"1~3\"\n","  elif (4 <= x <= 9):\n","    return \"4~9\"\n","  elif (10 <= x <= 18):\n","    return \"10~18\"\n","  elif (19 <= x <= 23):\n","    return \"19~23\"\n","\n","\n","def outliers_handeler(df,features):\n","  '''\n","  Function that handels outliers and put them in the range of the IQR\n","\n","  parameters:\n","    df(DataFrame)\n","    features(string)\n","  '''\n","  for feature in features:\n","    tenth_percentile = np.percentile(df[feature],10)\n","    ninetieth_percentile = np.percentile(df[feature],90)\n","    b = np.where(df[feature]< tenth_percentile, tenth_percentile, df[feature])\n","    b = np.where(b>ninetieth_percentile, ninetieth_percentile, b)\n","    df[feature] = pd.DataFrame(b)\n","\n","\n","def remove_duplicates(df):\n","  '''\n","  Function that removed duplicates\n","\n","  parameters:\n","    df(dataframe)\n","  '''\n","  print('nuber of duplicats = ',df[df.duplicated()==True].shape[0])\n","  print('assumed numbers of rows remaining = ',df.shape[0] - df[df.duplicated()==True].shape[0])\n","  df.drop_duplicates(keep = 'first', inplace = True)\n","  df.reset_index(inplace = True,drop=True)\n","  print('number of rows after removing the duplicates = ',df.shape[0])\n","\n","\n","def label_encoding(df,feature,categories):\n","  '''\n","  Funciton that label encode a feature accordning to specific order\n","\n","  Parameters:\n","    df(dataframe)\n","    feature(string)\n","    categories(list): a lsit of the categories in the received feature that wil be replaced in the same order\n","  '''\n","  for i,cat in enumerate (categories):\n","    df[feature].replace({cat:i},inplace = True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmCNZDyZtK40","executionInfo":{"status":"ok","timestamp":1707345341137,"user_tz":-120,"elapsed":407,"user":{"displayName":"Sarah Tarek","userId":"16775565530848637283"}},"outputId":"7ac6f35a-800d-41e3-be89-b1ef1d91e399"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing custom_funcs.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"csmGpZOstK7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6gyci88StK-K"},"execution_count":null,"outputs":[]}]}